# Import the Groq client library to interact with the Groq API
from groq import Groq
from apiKey import GROQ_API_KEY

# Initialize the Groq client
client = Groq(api_key=GROQ_API_KEY)

# Function to converse with a large language model (LLM) for movie recommendations
def converse_with_llm(prompt):
    """
    This function sends a prompt to a large language model (LLM) via the Groq API
    and retrieves a response. The LLM is configured to act as a movie recommendation assistant.

    Args:
        prompt (str): The user's input or question, e.g., "Recommend me a comedy movie."

    Returns:
        str: The response generated by the LLM, which could be a movie recommendation or related information.
    """
    
    # Create a chat completion request using the Groq client
    chat_completion = client.chat.completions.create(
        messages=[
            # System message to set the context for the LLM
            {"role": "system", "content": "You are a movie recommendation assistant."},
            
            # User message containing the input prompt
            {"role": "user", "content": prompt},
        ],
        
        #FINETUNING
        # Specify the model to use for generating responses
        model="llama-3.3-70b-versatile",
        
        # Control the randomness of the output (higher values make it more creative)
        temperature=0.7,
        
        # Limit the maximum number of tokens in the response
        max_tokens=1024,
        
        # Use nucleus sampling to control diversity of the output
        top_p=1,
        
        # Optionally specify stop sequences to end the response early (None means no stop sequence)
        stop=None,
        
        # Disable streaming of the response (stream=False means the entire response is returned at once)
        stream=False,
    )
    
    # Extract and return the content of the first response choice
    return chat_completion.choices[0].message.content